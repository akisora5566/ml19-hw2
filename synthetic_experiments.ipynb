{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed for MacOS compatibility on some systems\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "from linearclassifier import linear_predict, perceptron_update, plot_predictions, log_reg_train\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "\n",
    "num_dim = 2\n",
    "num_points = 200\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# On the Mac we tested this on, using this random seed produces a fairly even class balance\n",
    "# Python is not always consistent across machines with preserving seeded random behavior, \n",
    "# so if your histogram shows major class imbalance, change this seed to get better balance\n",
    "\n",
    "data = np.random.randn(num_dim, num_points)\n",
    "true_model = {'weights': np.random.randn(num_dim, num_classes)}\n",
    "\n",
    "labels = linear_predict(data, true_model)\n",
    "plt.hist(labels)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create noisy labels\n",
    "noise_index = np.random.rand(num_points) < 0.3\n",
    "noisy_labels = labels.copy()\n",
    "noisy_labels[noise_index] = np.random.randint(0, num_classes, np.count_nonzero(noise_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot generated data\n",
    "markers = ['xr', 'ob', '*m',  'dk']\n",
    "\n",
    "plt.subplot(121)\n",
    "for i in range(num_classes):\n",
    "    plt.plot(data[0, labels == i], data[1, labels == i], markers[i])\n",
    "plt.title('Separable Data')\n",
    "\n",
    "plt.subplot(122)\n",
    "for i in range(num_classes):\n",
    "    plt.plot(data[0, noisy_labels == i], data[1, noisy_labels == i], markers[i])\n",
    "plt.title('Noisy Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "\n",
    "num_train = int(num_points / 2)\n",
    "num_test = num_points - num_train\n",
    "\n",
    "train_data = data[:, :num_train]\n",
    "test_data = data[:, num_train + 1:]\n",
    "\n",
    "# store noiseless and noisy labels in tuples\n",
    "\n",
    "train_labels = (labels[:num_train], noisy_labels[:num_train])\n",
    "test_labels = (labels[num_train + 1:], noisy_labels[num_train + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perceptron experiment\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "train_accuracy = dict()\n",
    "test_accuracy = dict()\n",
    "model = dict()\n",
    "\n",
    "for is_noisy in (False, True):\n",
    "    model[is_noisy] = { 'weights': np.zeros((num_dim, num_classes)) }\n",
    "    train_accuracy[is_noisy] = np.zeros(epochs)\n",
    "    test_accuracy[is_noisy] = np.zeros(epochs)\n",
    " \n",
    "    for epoch in range(epochs):\n",
    "        # first measure training and testing accuracy            \n",
    "        predictions = linear_predict(train_data, model[is_noisy])\n",
    "        train_accuracy[is_noisy][epoch] = np.sum(predictions == train_labels[is_noisy]) / num_train\n",
    "\n",
    "        predictions = linear_predict(test_data, model[is_noisy])\n",
    "        test_accuracy[is_noisy][epoch] = np.sum(predictions == test_labels[is_noisy]) / num_test\n",
    "\n",
    "        # run perceptron training\n",
    "        mistakes = 0\n",
    "        for i in range(num_train):\n",
    "            correct = perceptron_update(train_data[:, i], model[is_noisy], train_labels[is_noisy][i])\n",
    "            \n",
    "            if not correct:\n",
    "                mistakes += 1\n",
    "        \n",
    "        print(\"Finished epoch %d with %d mistakes.\" % (epoch, mistakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot results of perceptron training\n",
    "\n",
    "plt.subplot(121)\n",
    "train_line = plt.plot(range(epochs), train_accuracy[False], label=\"Training\")\n",
    "test_line = plt.plot(range(epochs), test_accuracy[False], label=\"Testing\")\n",
    "plt.title('Separable Data')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "train_line = plt.plot(range(epochs), train_accuracy[True], label=\"Training\")\n",
    "test_line = plt.plot(range(epochs), test_accuracy[True], label=\"Testing\")\n",
    "plt.title('Noisy Data')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot clean predictions\n",
    "\n",
    "test_predictions = linear_predict(test_data, model[False])\n",
    "train_predictions = linear_predict(train_data, model[False])\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions(train_data, train_labels[False], train_predictions)\n",
    "plt.title('Training Data')\n",
    "plt.subplot(122)\n",
    "plot_predictions(test_data, test_labels[False], test_predictions)\n",
    "plt.title('Testing Data')\n",
    "\n",
    "print(\"Red markers indicate incorrect predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic regression gradient check\n",
    "\n",
    "# first check if the gradient and objective function are consistent with each other\n",
    "_ = log_reg_train(train_data, train_labels[True], {'lambda': 0.1}, \n",
    "              {'weights': np.random.randn(num_dim * num_classes)}, check_gradient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "\n",
    "lambda_vals = 10 ** np.linspace(-9, 2, 12)\n",
    "\n",
    "train_accuracy = dict()\n",
    "test_accuracy = dict()\n",
    "model = dict()\n",
    "\n",
    "for is_noisy in (False, True):\n",
    "    model[is_noisy] = {'weights': np.zeros((num_dim, num_classes))}\n",
    "    \n",
    "    train_accuracy[is_noisy] = np.zeros(lambda_vals.size)\n",
    "    test_accuracy[is_noisy] = np.zeros(lambda_vals.size)\n",
    "    \n",
    "    for i in range(lambda_vals.size):\n",
    "        params = {'lambda': lambda_vals[i]}\n",
    "        \n",
    "        model[is_noisy] = log_reg_train(train_data, train_labels[is_noisy], params, model[is_noisy])\n",
    "        \n",
    "        train_predictions = linear_predict(train_data, model[is_noisy])\n",
    "        train_accuracy[is_noisy][i] = np.sum(train_predictions == train_labels[is_noisy]) / num_train\n",
    "\n",
    "        test_predictions = linear_predict(test_data, model[is_noisy])\n",
    "        test_accuracy[is_noisy][i] = np.sum(test_predictions == test_labels[is_noisy]) / num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot results of logistic regression parameter sweep\n",
    "\n",
    "plt.subplot(121)\n",
    "train_line = plt.semilogx(lambda_vals, train_accuracy[False], label=\"Training\")\n",
    "test_line = plt.semilogx(lambda_vals, test_accuracy[False], label=\"Testing\")\n",
    "plt.title('Separable Data')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "train_line = plt.semilogx(lambda_vals, train_accuracy[True], label=\"Training\")\n",
    "test_line = plt.semilogx(lambda_vals, test_accuracy[True], label=\"Testing\")\n",
    "plt.title('Noisy Data')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plot clean predictions\n",
    "\n",
    "test_predictions = linear_predict(test_data, model[False])\n",
    "train_predictions = linear_predict(train_data, model[False])\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions(train_data, train_labels[False], train_predictions)\n",
    "plt.title('Training Data')\n",
    "plt.subplot(122)\n",
    "plot_predictions(test_data, test_labels[False], test_predictions)\n",
    "plt.title('Testing Data')\n",
    "\n",
    "print(\"Red markers indicate incorrect predictions\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
